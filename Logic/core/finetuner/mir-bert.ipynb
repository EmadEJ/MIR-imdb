{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8811892,"sourceType":"datasetVersion","datasetId":5300468}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModelForSequenceClassification, BertForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizerFast, BertTokenizer \nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom huggingface_hub import HfFolder, Repository\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T23:45:44.847185Z","iopub.execute_input":"2024-06-28T23:45:44.848188Z","iopub.status.idle":"2024-06-28T23:45:44.856382Z","shell.execute_reply.started":"2024-06-28T23:45:44.848141Z","shell.execute_reply":"2024-06-28T23:45:44.855259Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"class IMDbDataset(Dataset):\n    \"\"\"\n    A PyTorch dataset for the movie genre classification task.\n    \"\"\"\n\n    def __init__(self, encodings, labels):\n        \"\"\"\n        Initialize the IMDbDataset class.\n\n        Args:\n            encodings (dict): The tokenized input encodings.\n            labels (list): The corresponding labels.\n        \"\"\"\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get a single item from the dataset.\n\n        Args:\n            idx (int): The index of the item to retrieve.\n\n        Returns:\n            dict: A dictionary containing the input encodings and labels.\n        \"\"\"\n        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n#         print(self.labels[idx])\n        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        \n#         print(item)\n        return item\n\n    def __len__(self):\n        \"\"\"\n        Get the length of the dataset.\n\n        Returns:\n            int: The number of items in the dataset.\n        \"\"\"\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T23:45:44.863414Z","iopub.execute_input":"2024-06-28T23:45:44.863765Z","iopub.status.idle":"2024-06-28T23:45:44.875173Z","shell.execute_reply.started":"2024-06-28T23:45:44.863735Z","shell.execute_reply":"2024-06-28T23:45:44.874014Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"code","source":"class BERTFinetuner:\n    \"\"\"\n    A class for fine-tuning the BERT model on a movie genre classification task.\n    \"\"\"\n\n    def __init__(self, file_path, top_n_genres=5):\n        \"\"\"\n        Initialize the BERTFinetuner class.\n\n        Args:\n            file_path (str): The path to the JSON file containing the dataset.\n            top_n_genres (int): The number of top genres to consider.\n        \"\"\"\n        self.file_path = file_path\n        self.top_n_genres = top_n_genres\n\n    def load_dataset(self):\n        \"\"\"\n        Load the dataset from the JSON file.\n        \"\"\"\n        with open(self.file_path, \"r\") as FILE:\n            data = json.load(FILE)\n        \n        summaries = [x['first_page_summary'] for x in data]\n        genres = [x['genres'][0] if len(x['genres']) > 0 else np.nan for x in data]\n        \n        self.dataset = pd.DataFrame({'summary': summaries, 'genre': genres})\n\n    def preprocess_genre_distribution(self):\n        \"\"\"\n        Preprocess the dataset by filtering for the top n genres\n        \"\"\"\n        self.dataset.dropna(inplace=True)\n        \n        cnt = self.dataset['genre'].value_counts()\n        self.top_genres = list(cnt.index[:self.top_n_genres])\n\n        mask = self.dataset['genre'].isin(self.top_genres)\n        self.dataset = self.dataset[mask]\n        \n        for _, row in self.dataset.iterrows():\n            row['genre'] = self.top_genres.index(row['genre'])\n\n    def split_dataset(self, test_size=0.3, val_size=0.5):\n        \"\"\"\n        Split the dataset into train, validation, and test sets.\n\n        Args:\n            test_size (float): The proportion of the dataset to include in the test split.\n            val_size (float): The proportion of the dataset to include in the validation split.\n        \"\"\"\n        self.train_dataset, self.test_dataset = train_test_split(self.dataset, test_size=test_size)\n        self.train_dataset, self.valid_dataset = train_test_split(self.train_dataset, test_size=val_size)\n\n    def create_dataset(self, texts, labels):\n        \"\"\"\n        Create a PyTorch dataset from the given encodings and labels.\n\n        Args:\n            encodings (dict): The tokenized input encodings.\n            labels (list): The corresponding labels.\n\n        Returns:\n            IMDbDataset: A PyTorch dataset object.\n        \"\"\"\n        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=200)\n#         print(encodings.keys())\n#         for umm in encodings['attention_mask']:\n#             print(len(umm))\n        return IMDbDataset(encodings, list(labels))\n\n    def fine_tune_bert(self, epochs=5, batch_size=16, warmup_steps=500, weight_decay=0.01):\n        \"\"\"\n        Fine-tune the BERT model on the training data.\n\n        Args:\n            epochs (int): The number of training epochs.\n            batch_size (int): The batch size for training.\n            warmup_steps (int): The number of warmup steps for the learning rate scheduler.\n            weight_decay (float): The strength of weight decay regularization.\n        \"\"\"\n        train_data = self.create_dataset(self.train_dataset['summary'], self.train_dataset['genre'])\n        train_dataloader= DataLoader(dataset=train_data, batch_size=batch_size, sampler=RandomSampler(train_data))\n        eval_data = self.create_dataset(self.valid_dataset['summary'], self.valid_dataset['genre'])\n        eval_dataloader= DataLoader(dataset=eval_data, batch_size=batch_size)\n\n        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=self.top_n_genres)\n\n        optimizer = AdamW(self.model.parameters(), lr=1e-4, eps=1e-9, weight_decay=weight_decay)\n        \n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=(epochs * len(train_dataloader)))\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(device)\n        \n        for epoch in range(epochs):\n            self.model.train()\n            total_loss = 0\n            for batch in tqdm(train_dataloader):\n                # print(batch)\n                batch = {k: v.to(device) for k, v in batch.items()}\n                output = self.model(**batch)\n                loss = output.loss\n                total_loss += loss.item()\n                \n                optimizer.zero_grad()\n                loss.backward()\n                clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n            \n            avg_loss = total_loss / len(train_dataloader)\n            print(f\"Average training loss in epoch {epoch+1}/{epochs}: {avg_loss}\")\n        \n        self.model.save_pretrained(\"fine_tuned_bert\")\n        print(\"I'm Done!!!?\")\n\n    def compute_metrics(self, preds, trues):\n        \"\"\"\n        Compute evaluation metrics based on the predictions.\n\n        Args:\n            pred (EvalPrediction): The model's predictions.\n\n        Returns:\n            dict: A dictionary containing the computed metrics.\n        \"\"\"\n        precision, recall, f1, support = precision_recall_fscore_support(trues, preds, average='macro')\n        acc = accuracy_score(trues, preds)\n        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"support\": support}\n\n    def evaluate_model(self):\n        \"\"\"\n        Evaluate the fine-tuned model on the test set.\n        \"\"\"\n        test_data = self.create_dataset(self.test_dataset['summary'], self.test_dataset['genre'])        \n        test_dataloader = DataLoader(test_data, batch_size=16)\n    \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(device)\n        \n        self.model.eval()\n        \n        preds, trues = [], []\n        for batch in tqdm(test_dataloader):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.no_grad():\n                output = self.model(**batch)\n            \n            logits = output.logits\n            labels = batch[\"labels\"]\n            pred = torch.argmax(logits, dim=1).flatten().detach().cpu().numpy()\n            labels = labels.flatten().cpu().numpy()\n            preds.extend(pred)\n            trues.extend(labels)\n        \n        metrics = self.compute_metrics(preds, trues)\n        print(\"evaluation result:\\n\", metrics)\n            \n    def save_model(self, model_name):\n        \"\"\"\n        Save the fine-tuned model and tokenizer to the Hugging Face Hub.\n\n        Args:\n            model_name (str): The name of the model on the Hugging Face Hub.\n        \"\"\"\n        token = HfFolder.get_token()\n        if token is None:\n            raise ValueError(\"You must be logged into the Hugging Face Hub. Use `huggingface-cli login`.\")\n\n        repo = Repository(local_dir=model_name, clone_from=\"EmadEJ/MIR-Bert\", use_auth_token=True, git_email=\"s.emad.emamjomeh@gmail.com\")\n\n        self.model.save_pretrained(model_name)\n\n        repo.push_to_hub(commit_message=\"Update model\")  ","metadata":{"_uuid":"e53d3aa5-bf3d-441b-bc4e-0adf35ab90a5","_cell_guid":"6320c1de-e1d0-453d-b615-be2448bd0390","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-28T23:45:44.941659Z","iopub.execute_input":"2024-06-28T23:45:44.942044Z","iopub.status.idle":"2024-06-28T23:45:44.973798Z","shell.execute_reply.started":"2024-06-28T23:45:44.942011Z","shell.execute_reply":"2024-06-28T23:45:44.972458Z"},"trusted":true},"execution_count":238,"outputs":[]},{"cell_type":"code","source":"# Instantiate the class\n\nbert_finetuner = BERTFinetuner('/kaggle/input/imdb-crawled/their_IMDB_crawled.json', top_n_genres=5)\n\n# Load the dataset\nbert_finetuner.load_dataset()\n\n# Preprocess genre distribution\nbert_finetuner.preprocess_genre_distribution()\n\n# Split the dataset\nbert_finetuner.split_dataset()\n\n# Fine-tune BERT model\nbert_finetuner.fine_tune_bert(epochs=20)\n\n# Compute metrics\nbert_finetuner.evaluate_model()\n\n# Save the model (optional)\nbert_finetuner.save_model('Movie_Genre_Classifier')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T23:45:44.975673Z","iopub.execute_input":"2024-06-28T23:45:44.976012Z","iopub.status.idle":"2024-06-28T23:57:27.449466Z","shell.execute_reply.started":"2024-06-28T23:45:44.975984Z","shell.execute_reply":"2024-06-28T23:57:27.448043Z"},"trusted":true},"execution_count":239,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 158/158 [00:35<00:00,  4.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 1/20: 1.5008425161808352\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 2/20: 1.0285924808888496\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 3/20: 0.6699491040427473\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 4/20: 0.42151061263925665\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 5/20: 0.24974622032781946\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 6/20: 0.16360528908354596\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 7/20: 0.09265169762318093\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 8/20: 0.06231155201090541\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 9/20: 0.06184357420587028\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 10/20: 0.047672409571256326\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 11/20: 0.045777090079861374\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 12/20: 0.02701852953944155\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 13/20: 0.026039085539829285\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 14/20: 0.018928137105002504\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 15/20: 0.025029998337127903\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 16/20: 0.018503532834097603\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 17/20: 0.02058138186176517\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 18/20: 0.01755845782325288\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:33<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 19/20: 0.01725010125922876\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 158/158 [00:34<00:00,  4.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss in epoch 20/20: 0.017510176486338642\nI'm Done!!!?\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 136/136 [00:09<00:00, 14.65it/s]","output_type":"stream"},{"name":"stdout","text":"evaluation result:\n {'accuracy': 0.5716928769657724, 'f1': 0.5601180983366303, 'precision': 0.5633496541099147, 'recall': 0.5614727639673183, 'support': None}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[239], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m bert_finetuner\u001b[38;5;241m.\u001b[39mevaluate_model()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Save the model (optional)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mbert_finetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMovie_Genre_Classifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[238], line 170\u001b[0m, in \u001b[0;36mBERTFinetuner.save_model\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m    168\u001b[0m token \u001b[38;5;241m=\u001b[39m HfFolder\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must be logged into the Hugging Face Hub. Use `huggingface-cli login`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m repo \u001b[38;5;241m=\u001b[39m Repository(local_dir\u001b[38;5;241m=\u001b[39mmodel_name, clone_from\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmadEJ/MIR-Bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, git_email\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms.emad.emamjomeh@gmail.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(model_name)\n","\u001b[0;31mValueError\u001b[0m: You must be logged into the Hugging Face Hub. Use `huggingface-cli login`."],"ename":"ValueError","evalue":"You must be logged into the Hugging Face Hub. Use `huggingface-cli login`.","output_type":"error"}]}]}